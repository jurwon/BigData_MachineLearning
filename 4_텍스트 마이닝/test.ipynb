{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#감정분석, 토픽분석\n",
    "#샘플 데이터, 네이버 크롤러 수집된, 뉴스 기사, 영화 리뷰 데이터 등\n",
    "#\n",
    "#용어 정리\n",
    "#텍스트 마이닝\n",
    "#비정형 텍스트 데이터 패턴을 찾아내고 의미 있는 정보를 추출 및 분석 과정\n",
    "#과정 : 텍스트 전처리 -> 특성 벡터화 -> 머신 러닝 모델 구축 및 학습/평가 -> 시각화\n",
    "\n",
    "#특성 벡터화와 특성 추출\n",
    "#단어 기반의 특성 추출하고, 숫자 형 값인 벡터 값으로 표현\n",
    "#특성 벡터화 종류 -> 1)BoW(Bag of Words)와 Word2vec\n",
    "#1) BoW -> 문서가 가지고 있는 모든 단어에 대해 순서 무시하고, 빈도만 고려해서\n",
    "#해당 단어가 얼마나 자주 등장하는지를 특성 벡터로 만드는 방법\n",
    "\n",
    "#종류) a. 카운트 기반 벡터 b. TF-IDF(Term Frequency - Inverse Document Frequency)기반 벡터 방식\n",
    "\n",
    "# a) 카운트 기반 벡터\n",
    "# 단어 피처에 숫자형 값을 할당 할 시 , 각 문서에서 해당 단어가 등장하는 횟수,\n",
    "# 단어의 빈도를 부여하는 벡터화 방식(숫자형으로 변경한다.)\n",
    "# ex) 전체 문서에 등장한 단어를 기반으로 어휘 사전을 생성하고, 단어마다 등장 횟수를 카운트하여\n",
    "# 해당 단어의 정수 벡터 값으로 할당한다.\n",
    "# 문서별 단어의 빈도를 정리하는, 행렬 -> 문서 단어 행렬(DTM = Document Term Matrix)\n",
    "# 단어 출현 빈도가 높을수록 중요한 단어로 취급한다.\n",
    "# 문서 d에 등장하는 단어 t의 횟수 -> tf(t,d) 표기\n",
    "# 단어 행렬 : ex) 문서1 -> 사과:10, 점심:30, -> tf(\"점심\",문서1) = 30\n",
    "# 사이킷런에서 -> CountVectorize 모듈(파일)에서 제공함.\n",
    "\n",
    "\n",
    "# b) TF-IDF(Term Frequency - Inverse Document Frequency)기반 벡터 방식\n",
    "# 카운트 기반에서, 단순 해당 단어 빈도가 많으면 그냥 중요하다고 단순 판단했음.\n",
    "# but 해당 단어가 단지 문장 구성상 많이 사용하는 단어 일 가능성 있음.\n",
    "#  TF-IDF는 특정 문서에 많이 나타나는 단어는 해당 문서의 단어 벡터에 가중치를 높이고\n",
    "# 모든 문서에 많이 나타나는 단어는 문서의 특징을 나타내는 단어가 아니고, 그냥 범용적으로 사용되는 단어.\n",
    "# -> 가중치 낮추는 방식\n",
    "# 문서 d에 등장한 단어 t의  TF-IDF 표현식\n",
    "# tf - idf(t,d) = tf(t,d) * idf(t,d)\n",
    "# tf(t,d) : 문서 d에 등장하는 단어 t의 횟수\n",
    "# idf(t,d) : 역문서 빈도\n",
    "# 사이킷런에서 TfidfVectorizer 모듈에서 제공됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
